---
layout: post
title:  'Reading 11: Artificial Intelligence'
date:   2018-04-08 20:00:00 -0500
categories: jekyll update
---
This [Computer World article][computer-world] defines artificial intelligence as follows: “Simply put, artificial intelligence is a sub-field of computer science. Its goal is to enable the development of computers that are able to do things normally done by people -- in particular, things associated with people acting intelligently.”  There is often a confusion that artificial intelligence is making computers intelligent like humans, but this is not entirely true.  AI is making computers do things that require human intelligence, regardless if the computer is doing it in a manner that resembles intelligence.  “Strong AI” does try to mimic human intelligence when completing these tasks, while “weak AI” focuses on getting the job done, making models get results as humans would but do not think like humans.  Many successful models fall in an area between the two. There are also the notions of “narrow AI,” which is good at specific tasks, and “general AI,” which can reason about things in general.

I am certainly no expert on the underworkings of AlphaGo, Deep Blue, Watson, and AlphaZero, but I would argue they are examples of weak and narrow AI.  They satisfy the above definition by performing specific human tasks and performing these tasks incredibly well.  Yet, Watson’s superiority comes from its ability to query the entire internet and find the exact answers he needs. Answering the *Jeopardy!* "US Cities" prompt, “Its largest airport is named for a World War II hero; its second largest, for a World War II battle” with "What is Toronto?" shows it does not quite think through things as humans would.  That being said, the underlying technologies in these systems could reflect some stronger AI techniques and could have useful implications when it comes to general AI.  For example, AlphaZero’s ability to learn chess in four hours has huge implications for what machines can learn and how quickly, regardless if they are learning the same way humans would.

I do not believe the Turing Test is a valid measure intelligence, at least in the sense that Turing meant it to be.  I would not say that the Eugene Goostman chatbot that passed the Turing Test is truly intelligent.  It definitely is artificially intelligent and can manage to give results to tasks that require human intelligence (like holding a conversation) but because the chatbot was built on weak AI, it does not “think” as we would describe the word.  The Chinese Room is a great counterargument: someone who does not know Chinese could potentially follow the algorithm used by a Chinese AI communicator and output Chinese manually but he or she still would not know Chinese, thus, the computer cannot have understood Chinese.  The counterargument to the Chinese Room would be that if the algorithm was one such that the manual translator finished his processing and did in fact end up having an understanding of Chinese then perhaps similarly the computer learned Chinese during the process.  Such an algorithm could resemble strong AI.

I break fears about AI into two categories: one fear is that AI will replace human jobs and we will face an unprecedented job shortage, the other fear is that AI will take over the world and kill the human race.  I think we’ll talk more about the first fear when we talk about automation, and I think it is a valid fear and very plausible.  Weak and narrow AI has been performing phenomenally at human tasks, and that is where the field is excelling right now.  We should not stop making AI because of these fears because AI has so many benefits and will make everyone’s live easier, but we need to be prepared for what is to come.  The second fear is a long way away; if we ever reach that point.  This AI would need to be more strong and general. People often think it is just the idea that a robot learns how to plug itself in when its low on battery or learns it might “feel” better (if our robots can ever feel) if it is not performing slave labor for its builder.  I think more realistic cases are powerful AI that does not even need to be robotic that is used in cyber and social warfare that ends up having malicious outcomes for humanity. 

I have wondered quite a bit about consciousness and what it means to be alive (as I am sure most people have).  We always wonder if computer minds can be comparable to human minds.  But humans are not the only species with minds.  Do insects have minds? Would a robot that is as sophisticated as a lice organism thus have a mind? If it is programmed to think the same way as the bug, it’s hard to argue why not.  Morality? I wonder if the creator would assume moral responsibility for their AI, or if the AI would assume its own moral responsibility.  I would say moral responsibility lies with the creator unless AI reaches a really high level of complexity.  I do not want to think that we are biological computers, but sometimes I do think we are.  We definitely want to think that we are special and not just a a product of a sophisticated algorithm.  Ethical implications of these things are that we can potentially make other beings that we may or may not be responsible for.  It also leads us to question our own spot in the universe, and if we are robots ourselves, our own moral responsibility. 

[computer-world]: https://www.computerworld.com/article/2906336/emerging-technology/what-is-artificial-intelligence.html
